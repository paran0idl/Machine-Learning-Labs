# Tree ROC Curve
TPR_tree<-c()
FPR_tree<-c()
mis_rate<-c()
for(pi in seq(0.05,0.95,0.05)){
res<-ifelse(pred_prunedTree[,2]>pi,1,0)
actual<-ifelse(test$good_bad=="good",1,0)
#u=union(res,actual)
factor(res,levels = c(0,1))
cft<-table(factor(res,levels = c(0,1)),factor(actual))
TP <- cft[2, 2]
TN <- cft[1, 1]
FP <- cft[2, 1]
FN <- cft[1, 2]
current_TPR<-TP/(TP+FN)
current_FPR<-FP/(FP+TN)
TPR<-c(TPR,current_TPR)
FPR<-c(FPR,current_FPR)
}
ggplot()+geom_line(aes(x=FPR,y=TPR))+geom_point(aes(x=FPR,y=TPR))
# Naive Bayes
TPR<-c()
FPR<-c()
for(pi in seq(0.05,0.95,0.05)){
res<-ifelse(pred_bayes_test[,2]>pi,1,0)
actual<-ifelse(test$good_bad=="good",1,0)
#u=union(res,actual)
factor(res,levels = c(0,1))
cft<-table(factor(res,levels = c(0,1)),factor(actual))
TP <- cft[2, 2]
TN <- cft[1, 1]
FP <- cft[2, 1]
FN <- cft[1, 2]
current_TPR<-TP/(TP+FN)
current_FPR<-FP/(FP+TN)
TPR<-c(TPR,current_TPR)
FPR<-c(FPR,current_FPR)
}
TPR
FPR
ggplot()+geom_line(aes(x=FPR,y=TPR))+geom_point(aes(x=FPR,y=TPR))
prunedTree=prune.tree(tree_model,best=4)
pred_prunedTree=predict(prunedTree,test)
pred_bayes_train=predict(naive_bayes_model,train,type="raw")
pred_bayes_test=predict(naive_bayes_model,test,type="raw")
TPR_tree<-c()
FPR_tree<-c()
for(pi in seq(0.05,0.95,0.05)){
res<-ifelse(pred_prunedTree[,2]>pi,1,0)
actual<-ifelse(test$good_bad=="good",1,0)
factor(res,levels = c(0,1))
cft<-table(factor(res,levels = c(0,1)),factor(actual))
TP <- cft[2, 2]
TN <- cft[1, 1]
FP <- cft[2, 1]
FN <- cft[1, 2]
current_TPR<-TP/(TP+FN)
current_FPR<-FP/(FP+TN)
TPR_tree<-c(TPR_tree,current_TPR)
FPR_tree<-c(FPR_tree,current_FPR)
}
print(TPR_tree)
print(FPR_tree)
TPR_nb<-c()
FPR_nb<-c()
for(pi in seq(0.05,0.95,0.05)){
res<-ifelse(pred_bayes_test[,2]>pi,1,0)
actual<-ifelse(test$good_bad=="good",1,0)
factor(res,levels = c(0,1))
cft<-table(factor(res,levels = c(0,1)),factor(actual))
TP <- cft[2, 2]
TN <- cft[1, 1]
FP <- cft[2, 1]
FN <- cft[1, 2]
current_TPR<-TP/(TP+FN)
current_FPR<-FP/(FP+TN)
TPR_nb<-c(TPR_nb,current_TPR)
FPR_nb<-c(FPR_nb,current_FPR)
}
print(TPR_nb)
print(FPR_nb)
ggplot()+geom_line(aes(x=FPR_nb,y=TPR_nb),col="red")+geom_point(aes(x=FPR_nb,y=TPR_nb),col="red")+geom_line(aes(x=FPR_tree,y=TPR_tree),col="blue")+geom_point(aes(x=FPR_tree,y=TPR_tree),col="blue")+xlab("FPR")+ylab("TPR")
regression_tree=tree(EX~MET,data=data,control = tree.control(minsize = 8,nobs = nrow(data)))
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(tree)
library(ggplot2)
library(boot)
RNGversion('3.5.1')
set.seed(12345)
RNGversion("3.5.1")
data<-read_xls("data/creditscoring.xls")
data$good_bad<-as.factor(data$good_bad)
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
id1=setdiff(1:n, id)
set.seed(12345)
id2=sample(id1, floor(n*0.25))
valid=data[id2,]
id3=setdiff(id1,id2)
test=data[id3,]
get_misclassfication_rate<-function(pred,actual){
cft<-table(pred,actual)
print(cft)
rate<-(cft[1,2]+cft[2,1])/sum(cft)*100
print(rate)
}
create_tree_model<-function(measure){
tree_model=tree(good_bad~.,data=train,split = c(measure))
pred_train=predict(tree_model,train,type="class")
pred_test=predict(tree_model,test,type="class")
cat("using ",measure,"as measure\n")
cat("\n Confusion matrix and misclassfication rate for train data")
get_misclassfication_rate(pred_train,train$good_bad)
cat("\n Confusion matrix and misclassfication rate for test data\n")
get_misclassfication_rate(pred_test,test$good_bad)
}
create_tree_model("deviance")
create_tree_model("gini")
tree_model=tree(good_bad~.,data=train,split = c("deviance"))
trainScore=rep(0,15)
testScore=rep(0,15)
for(i in 2:15){
prunedTree=prune.tree(tree_model,best=i)
pred=predict(prunedTree, newdata=valid, type="tree")
trainScore[i]=deviance(prunedTree)
testScore[i]=deviance(pred)
}
ggplot()+geom_point(aes(x=c(2:15),y=trainScore[2:15]),col="red")+
geom_point(aes(x=c(2:15),y=testScore[2:15]),col="blue")+
scale_x_discrete(limits=c(2:15))+xlab("leaves")+ylab("score")
prunedTree=prune.tree(tree_model,best=4)
summary(prunedTree)
plot(prunedTree)
text(prunedTree,pretty = 0)
pred_prunedTree=predict(prunedTree,test,type = "class")
get_misclassfication_rate(pred_prunedTree,test$good_bad)
library(e1071)
naive_bayes_model=naiveBayes(good_bad~.,train)
pred_bayes_train=predict(naive_bayes_model,train)
cat("\n Confusion matrix and misclassfication rate for train data")
get_misclassfication_rate(pred_bayes_train,train$good_bad)
pred_bayes_test=predict(naive_bayes_model,test)
cat("\n Confusion matrix and misclassfication rate for test data")
get_misclassfication_rate(pred_bayes_test,test$good_bad)
prunedTree=prune.tree(tree_model,best=4)
pred_prunedTree=predict(prunedTree,test)
pred_bayes_train=predict(naive_bayes_model,train,type="raw")
pred_bayes_test=predict(naive_bayes_model,test,type="raw")
TPR_tree<-c()
FPR_tree<-c()
for(pi in seq(0.05,0.95,0.05)){
res<-ifelse(pred_prunedTree[,2]>pi,1,0)
actual<-ifelse(test$good_bad=="good",1,0)
factor(res,levels = c(0,1))
cft<-table(factor(res,levels = c(0,1)),factor(actual))
TP <- cft[2, 2]
TN <- cft[1, 1]
FP <- cft[2, 1]
FN <- cft[1, 2]
current_TPR<-TP/(TP+FN)
current_FPR<-FP/(FP+TN)
TPR_tree<-c(TPR_tree,current_TPR)
FPR_tree<-c(FPR_tree,current_FPR)
}
TPR_nb<-c()
FPR_nb<-c()
for(pi in seq(0.05,0.95,0.05)){
res<-ifelse(pred_bayes_test[,2]>pi,1,0)
actual<-ifelse(test$good_bad=="good",1,0)
factor(res,levels = c(0,1))
cft<-table(factor(res,levels = c(0,1)),factor(actual))
TP <- cft[2, 2]
TN <- cft[1, 1]
FP <- cft[2, 1]
FN <- cft[1, 2]
current_TPR<-TP/(TP+FN)
current_FPR<-FP/(FP+TN)
TPR_nb<-c(TPR_nb,current_TPR)
FPR_nb<-c(FPR_nb,current_FPR)
}
ggplot()+geom_line(aes(x=FPR_nb,y=TPR_nb),col="red")+geom_point(aes(x=FPR_nb,y=TPR_nb),col="red")+geom_line(aes(x=FPR_tree,y=TPR_tree),col="blue")+geom_point(aes(x=FPR_tree,y=TPR_tree),col="blue")+xlab("FPR")+ylab("TPR")
loss_matrix<-matrix(c(0,10,1,0),ncol=2)
res_train<-c()
for(i in c(1:dim(pred_bayes_train)[1])){
if((pred_bayes_train[i,1]/pred_bayes_train[i,2])>(loss_matrix[2,1]/loss_matrix[1,2])){
res_train<-c(res_train,0)
}else{
res_train<-c(res_train,1)
}
}
cat("\n Confusion matrix and misclassfication rate for train data\n")
get_misclassfication_rate(res_train,train$good_bad)
res_test<-c()
for(i in c(1:dim(pred_bayes_test)[1])){
if((pred_bayes_test[i,1]/pred_bayes_test[i,2])>(loss_matrix[2,1]/loss_matrix[1,2])){
res_test<-c(res_test,0)
}else{
res_test<-c(res_test,1)
}
}
cat("\n Confusion matrix and misclassfication rate for test data\n")
get_misclassfication_rate(res_test,test$good_bad)
RNGversion("3.5.1")
data<-read.csv("data/State.csv",header = TRUE,sep=";",dec = ",")
set.seed(12345)
data<-data[order(data$MET),]
tree_mod<-tree(EX~MET,data=data,control = tree.control(minsize = 8,nobs = nrow(data)))
cv.res=cv.tree(tree_mod)
plot(cv.res$size, cv.res$dev, type="b", col="red")
plot(log(cv.res$size), cv.res$dev, type="b", col="red")
prunedTree=prune.tree(tree_mod,best=3)
pred=predict(prunedTree,data=data)
ggplot()+geom_line(aes(x=c(1:length(pred)),y=pred),col="red")+geom_point(aes(x=c(1:length(data$EX)),y=data$EX),col="blue")
res=data$EX-pred
hist(res)
RNGversion("3.5.1")
data<-read.csv("data/State.csv",header = TRUE,sep=";",dec = ",")
set.seed(12345)
data<-data[order(data$MET),]
?fastICA
library(fastICA)
?fastICA
data<-read.csv("data/NIRSpectra.csv",header = TRUE,sep=";",dec = ",")
data$Viscosity=c()
res=prcomp(data)
print(res$x)
lambda=res$sdev^2
#eigenvalues
#proportion of variation
lambda=lambda/sum(lambda)*100
library(ggplot2)
df<-data.frame(PC=1:5,va=lambda[1:5])
ggplot(data=df,aes(x=PC,y=va))+geom_bar(stat = "identity")
sum(lambda[1:2])
# 2 PC
ggplot(data=data,aes(x=res$x[,1],y=res$x[,2]))+geom_point()
plot(x=res$x[,1],y=res$x[,2])
text(label=rownames(res$x[,1]))
u=res$rotation
plot(u[,1],main="PCA Traceplot, PC1")
plot(u[,2],main="PCA Traceplot, PC2")
library(fastICA)
set.seed(12345)
IC<-fastICA(data,n.comp = 2)
IC$K
IC$W
new_W<-IC$K%*%IC$W
plot(new_W[,1],main="ICA Traceplot, PC1")
plot(new_W[,2],main="ICA Traceplot, PC2")
plot(IC$S[,1],IC$S[,2])
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(tree)
library(ggplot2)
library(boot)
library(fastICA)
RNGversion('3.5.1')
set.seed(12345)
IC<-fastICA(data,n.comp = 2)
new_W<-IC$K%*%IC$W
plot(new_W[,1],main="ICA Traceplot, PC1")
plot(new_W[,2],main="ICA Traceplot, PC2")
plot(IC$S[,1],IC$S[,2])
#eigenvalues
lambda=res$sdev^2
data<-read.csv("data/NIRSpectra.csv",header = TRUE,sep=";",dec = ",")
data$Viscosity=c()
res=prcomp(data)
#eigenvalues
lambda=res$sdev^2
lambda=lambda/sum(lambda)*100
#df<-data.frame(PC=1:5,va=lambda[1:5])
df<-data.frame(PC=c(1:length(lambda)),va=lambda)
ggplot(data=df,aes(x=PC,y=va))+geom_bar(stat = "identity")+geom_text(mapping=aes(label=round(va,1)),size=2)
df1<-data.frame(x=res$x[,1],y=res$x[,2],n=1:length(res$x[,1]))
#ggplot(data=data,aes(x=res$x[,1],y=res$x[,2]))+geom_point()
#plot(x=res$x[,1],y=res$x[,2])
ggplot(data=df1,aes(x=x,y=y))+geom_point(col="white")+geom_text(label=(df1$n),size=2,col="black")
plot(IC$S[,1],IC$S[,2])
IC<-fastICA(data,n.comp = 2)
new_W<-IC$K%*%IC$W
plot(new_W[,1],main="ICA Traceplot, PC1")
plot(new_W[,2],main="ICA Traceplot, PC2")
plot(IC$S[,1],IC$S[,2])
plot(IC$S[,1],IC$S[,2])
plot(IC$S[,1],IC$S[,2])
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(tree)
library(ggplot2)
library(boot)
library(fastICA)
RNGversion('3.5.1')
set.seed(12345)
RNGversion("3.5.1")
data<-read_xls("data/creditscoring.xls")
data$good_bad<-as.factor(data$good_bad)
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
id1=setdiff(1:n, id)
set.seed(12345)
id2=sample(id1, floor(n*0.25))
valid=data[id2,]
id3=setdiff(id1,id2)
test=data[id3,]
get_misclassfication_rate<-function(pred,actual){
cft<-table(pred,actual)
print(cft)
rate<-(cft[1,2]+cft[2,1])/sum(cft)*100
print(rate)
}
create_tree_model<-function(measure){
tree_model=tree(good_bad~.,data=train,split = c(measure))
pred_train=predict(tree_model,train,type="class")
pred_test=predict(tree_model,test,type="class")
cat("using ",measure,"as measure\n")
cat("\n Confusion matrix and misclassfication rate for train data")
get_misclassfication_rate(pred_train,train$good_bad)
cat("\n Confusion matrix and misclassfication rate for test data\n")
get_misclassfication_rate(pred_test,test$good_bad)
}
create_tree_model("deviance")
create_tree_model("gini")
tree_model=tree(good_bad~.,data=train,split = c("deviance"))
trainScore=rep(0,15)
testScore=rep(0,15)
for(i in 2:15){
prunedTree=prune.tree(tree_model,best=i)
pred=predict(prunedTree, newdata=valid, type="tree")
trainScore[i]=deviance(prunedTree)
testScore[i]=deviance(pred)
}
ggplot()+geom_point(aes(x=c(2:15),y=trainScore[2:15]),col="red")+
geom_point(aes(x=c(2:15),y=testScore[2:15]),col="blue")+
scale_x_discrete(limits=c(2:15))+xlab("leaves")+ylab("score")
prunedTree=prune.tree(tree_model,best=4)
summary(prunedTree)
plot(prunedTree)
text(prunedTree,pretty = 0)
pred_prunedTree=predict(prunedTree,test,type = "class")
get_misclassfication_rate(pred_prunedTree,test$good_bad)
library(e1071)
naive_bayes_model=naiveBayes(good_bad~.,train)
pred_bayes_train=predict(naive_bayes_model,train)
cat("\n Confusion matrix and misclassfication rate for train data")
get_misclassfication_rate(pred_bayes_train,train$good_bad)
pred_bayes_test=predict(naive_bayes_model,test)
cat("\n Confusion matrix and misclassfication rate for test data")
get_misclassfication_rate(pred_bayes_test,test$good_bad)
prunedTree=prune.tree(tree_model,best=4)
pred_prunedTree=predict(prunedTree,test)
pred_bayes_train=predict(naive_bayes_model,train,type="raw")
pred_bayes_test=predict(naive_bayes_model,test,type="raw")
TPR_tree<-c()
FPR_tree<-c()
for(pi in seq(0.05,0.95,0.05)){
res<-ifelse(pred_prunedTree[,2]>pi,1,0)
actual<-ifelse(test$good_bad=="good",1,0)
factor(res,levels = c(0,1))
cft<-table(factor(res,levels = c(0,1)),factor(actual))
TP <- cft[2, 2]
TN <- cft[1, 1]
FP <- cft[2, 1]
FN <- cft[1, 2]
current_TPR<-TP/(TP+FN)
current_FPR<-FP/(FP+TN)
TPR_tree<-c(TPR_tree,current_TPR)
FPR_tree<-c(FPR_tree,current_FPR)
}
TPR_nb<-c()
FPR_nb<-c()
for(pi in seq(0.05,0.95,0.05)){
res<-ifelse(pred_bayes_test[,2]>pi,1,0)
actual<-ifelse(test$good_bad=="good",1,0)
factor(res,levels = c(0,1))
cft<-table(factor(res,levels = c(0,1)),factor(actual))
TP <- cft[2, 2]
TN <- cft[1, 1]
FP <- cft[2, 1]
FN <- cft[1, 2]
current_TPR<-TP/(TP+FN)
current_FPR<-FP/(FP+TN)
TPR_nb<-c(TPR_nb,current_TPR)
FPR_nb<-c(FPR_nb,current_FPR)
}
ggplot()+geom_line(aes(x=FPR_nb,y=TPR_nb),col="red")+geom_point(aes(x=FPR_nb,y=TPR_nb),col="red")+geom_line(aes(x=FPR_tree,y=TPR_tree),col="blue")+geom_point(aes(x=FPR_tree,y=TPR_tree),col="blue")+xlab("FPR")+ylab("TPR")
loss_matrix<-matrix(c(0,10,1,0),ncol=2)
res_train<-c()
for(i in c(1:dim(pred_bayes_train)[1])){
if((pred_bayes_train[i,1]/pred_bayes_train[i,2])>(loss_matrix[2,1]/loss_matrix[1,2])){
res_train<-c(res_train,0)
}else{
res_train<-c(res_train,1)
}
}
cat("\n Confusion matrix and misclassfication rate for train data\n")
get_misclassfication_rate(res_train,train$good_bad)
res_test<-c()
for(i in c(1:dim(pred_bayes_test)[1])){
if((pred_bayes_test[i,1]/pred_bayes_test[i,2])>(loss_matrix[2,1]/loss_matrix[1,2])){
res_test<-c(res_test,0)
}else{
res_test<-c(res_test,1)
}
}
cat("\n Confusion matrix and misclassfication rate for test data\n")
get_misclassfication_rate(res_test,test$good_bad)
RNGversion("3.5.1")
data<-read.csv("data/State.csv",header = TRUE,sep=";",dec = ",")
set.seed(12345)
data<-data[order(data$MET),]
tree_mod<-tree(EX~MET,data=data,control = tree.control(minsize = 8,nobs = nrow(data)))
cv.res=cv.tree(tree_mod)
plot(cv.res$size, cv.res$dev, type="b", col="red")
plot(log(cv.res$size), cv.res$dev, type="b", col="red")
prunedTree=prune.tree(tree_mod,best=3)
pred=predict(prunedTree,data=data)
ggplot()+geom_line(aes(x=c(1:length(pred)),y=pred),col="red")+geom_point(aes(x=c(1:length(data$EX)),y=data$EX),col="blue")
res=data$EX-pred
hist(res)
mean(res)
sd(res)
f=function(data,ind){
data1=data[ind,]
tree_mod<-tree(EX~MET,data=data1,control = tree.control(minsize = 8,nobs = nrow(data)))
prunedTree=prune.tree(tree_mod,best=3)
pred=predict(prunedTree,data)
return(pred)
}
boot_res=boot(data,f,R=1000)
CE<-envelope(boot_res,level = 0.95)
tree_mod<-tree(EX~MET,data=data,control = tree.control(minsize = 8,nobs = nrow(data)))
prunedTree=prune.tree(tree_mod,best=3)
pred=predict(prunedTree,data)
CI_band=data.frame(upper=CE$point[1,],lower=CE$point[2,],EX=data$EX,MET=data$MET,pred=pred)
ggplot(data=CI_band)+geom_point(aes(x=MET,y=EX))+geom_line(aes(x=MET,y=pred),col="blue")+geom_ribbon(aes(x=MET,ymin=lower,ymax=upper),alpha=0.1)
regression_tree=tree(EX~MET,data=data,control = tree.control(minsize = 8,nobs = nrow(data)))
mle=prune.tree(regression_tree,best=3)
rng<-function(data,mle){
data1=data.frame(EX=data$EX,MET=data$MET)
data1$EX=rnorm(length(data1$EX),predict(mle,data1),sd=sd(residuals(mle)))
return(data1)
}
f1<-function(data1){
new_tree<-tree(EX~MET,data=data1,control = tree.control(minsize = 8,nobs = nrow(data)))
new_prunedTree=prune.tree(new_tree,best=3)
new_pred<-predict(new_prunedTree,data)
new_pred<-rnorm(length(new_pred),mean=new_pred,sd=sd(residuals(mle)))
return(new_pred)
}
boot_res1=boot(data,statistic = f1,R=1000,mle = prunedTree,ran.gen = rng,sim="parametric")
CE<-envelope(boot_res1,level = 0.95)
tree_mod<-tree(EX~MET,data=data,control = tree.control(minsize = 8,nobs = nrow(data)))
prunedTree=prune.tree(tree_mod,best=3)
pred=predict(prunedTree,data)
CI_band=data.frame(upper=CE$point[1,],lower=CE$point[2,],EX=data$EX,MET=data$MET,pred=pred)
ggplot(data=CI_band)+geom_point(aes(x=MET,y=EX))+geom_line(aes(x=MET,y=pred),col="blue")+geom_ribbon(aes(x=MET,ymin=lower,ymax=upper),alpha=0.1)
data<-read.csv("data/NIRSpectra.csv",header = TRUE,sep=";",dec = ",")
data$Viscosity=c()
res=prcomp(data)
#eigenvalues
lambda=res$sdev^2
lambda=lambda/sum(lambda)*100
#df<-data.frame(PC=1:5,va=lambda[1:5])
df<-data.frame(PC=c(1:length(lambda)),va=lambda)
ggplot(data=df,aes(x=PC,y=va))+geom_bar(stat = "identity")+geom_text(mapping=aes(label=round(va,1)),size=2)
df1<-data.frame(x=res$x[,1],y=res$x[,2],n=1:length(res$x[,1]))
#ggplot(data=data,aes(x=res$x[,1],y=res$x[,2]))+geom_point()
#plot(x=res$x[,1],y=res$x[,2])
ggplot(data=df1,aes(x=x,y=y))+geom_point(col="white")+geom_text(label=(df1$n),size=2,col="black")
u=res$rotation
df<-data.frame(u1=u[,1],u2=u[,2],n=c(1:length(u[,1])))
ggplot(df,aes(x=n,y=u1))+geom_point(col="white")+geom_text(label=df$n,size=2)
ggplot(df,aes(x=n,y=u2))+geom_point(col="white")+geom_text(label=df$n,size=2)
#plot(u[,1],main="Traceplot, PC1")
#plot(u[,2],main="Traceplot, PC2")
IC<-fastICA(data,n.comp = 2)
new_W<-IC$K%*%IC$W
plot(new_W[,1],main="ICA Traceplot, PC1")
plot(new_W[,2],main="ICA Traceplot, PC2")
plot(IC$S[,1],IC$S[,2])
plot(IC$S[,1],IC$S[,2])
plot(IC$S[,1],IC$S[,2])
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(tree)
library(ggplot2)
library(boot)
library(fastICA)
RNGversion('3.5.1')
set.seed(12345)
u=res$rotation
df<-data.frame(u1=u[,1],u2=u[,2],n=c(1:length(u[,1])))
ggplot(df,aes(x=n,y=u1))+geom_point(col="white")+geom_text(label=df$n,size=2)
ggplot(df,aes(x=n,y=u2))+geom_point(col="white")+geom_text(label=df$n,size=2)
#plot(u[,1],main="Traceplot, PC1")
#plot(u[,2],main="Traceplot, PC2")
# u=res$rotation
# df<-data.frame(u1=u[,1],u2=u[,2],n=c(1:length(u[,1])))
# ggplot(df,aes(x=n,y=u1))+geom_point(col="white")+geom_text(label=df$n,size=2)
# ggplot(df,aes(x=n,y=u2))+geom_point(col="white")+geom_text(label=df$n,size=2)
U <- res$rotation
x <- 1:nrow(res$rotation)
ggplot() + geom_point(aes(x=x, y=U[,1])) + ggtitle("PCA - Traceplot PC1") +
xlab("Index") + ylab("Load")
ggplot() + geom_point(aes(x=x, y=U[,2])) + ggtitle("PCA - Traceplot PC2") +
xlab("Index") + ylab("Load")
